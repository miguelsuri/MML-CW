{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "dimensionality = [9, 12, 15, 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DynamicNeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, dim, hidden_sizes, dropout_rate):\n",
    "        super(DynamicNeuralNetwork, self).__init__()\n",
    "        layers = []\n",
    "        last_size = dim\n",
    "        for size in hidden_sizes:\n",
    "            layers.append(torch.nn.Linear(last_size, size))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            layers.append(torch.nn.Dropout(dropout_rate))\n",
    "            last_size = size\n",
    "        layers.append(torch.nn.Linear(last_size, 1))\n",
    "        layers.append(torch.nn.Sigmoid())\n",
    "        self.network = torch.nn.Sequential(*layers)\n",
    "\n",
    "        # Initialize weights, based on https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/Basics/pytorch_init_weights.py\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                torch.nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "                torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def denoise_bernoulli_data(X):\n",
    "    \"\"\"Denoise data by converting to binary values\"\"\"\n",
    "    return (X >= 0.5).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_dataloaders(X, y, batch_size):\n",
    "    \"\"\"Convert numpy arrays to PyTorch tensors and create DataLoaders.\"\"\"\n",
    "    X = denoise_bernoulli_data(X)\n",
    "    X = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "    y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "\n",
    "    dataset = TensorDataset(X, y)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, loss_fn, optimizer, num_epochs, device, early_stopping_patience=50, print_statements=False):\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "        # Train Model\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels.unsqueeze(1).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predicted = (outputs >= 0.5).float()\n",
    "            train_correct += (predicted == labels.unsqueeze(1)).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_accuracy = train_correct / train_total * 100\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Evaluate the model\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels.unsqueeze(1).float())\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                predicted = (outputs >= 0.5).float()\n",
    "                val_correct += (predicted == labels.unsqueeze(1)).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        \n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "\n",
    "        if patience_counter >= early_stopping_patience and print_statements:\n",
    "            print(f'Early stopping at epoch {epoch + 1}')\n",
    "            break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0 and print_statements:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    return model, best_val_loss, train_losses, val_losses, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, dim, test_loader, save_file):\n",
    "\tmodel.eval()\n",
    "\tall_preds = []\n",
    "\tall_labels = []\n",
    "    \n",
    "\twith torch.no_grad():\n",
    "\t\tfor inputs, labels in test_loader:\n",
    "\t\t\t# Ensure inputs are in the correct shape\n",
    "\t\t\tinputs = inputs.view(inputs.size(0), -1)  # Flatten if necessary\n",
    "\t\t\toutputs = model(inputs)\n",
    "\t\t\tpredictions = (outputs >= 0.5).float().cpu().numpy()\n",
    "\t\t\tall_preds.extend(predictions)\n",
    "\t\t\tall_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "\tall_preds = np.array(all_preds).squeeze()\n",
    "\tprint(all_preds)\n",
    "\t\n",
    "\tif save_file:    \n",
    "\t\tall_labels = np.array(all_labels)\n",
    "\t\tnp.save(f'kryptonite_{dim}_pred_nn.npy', all_preds)\n",
    "\n",
    "\treturn {\n",
    "        'accuracy': accuracy_score(all_labels, all_preds),\n",
    "        'precision': precision_score(all_labels, all_preds),\n",
    "        'recall': recall_score(all_labels, all_preds),\n",
    "        'f1': f1_score(all_labels, all_preds)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, test_loader):\n",
    "\tmodel.eval()\n",
    "\tall_preds = []\n",
    "\tall_labels = []\n",
    "    \n",
    "\twith torch.no_grad():\n",
    "\t\tfor inputs, labels in test_loader:\n",
    "\t\t\t# Ensure inputs are in the correct shape\n",
    "\t\t\tinputs = inputs.view(inputs.size(0), -1)  # Flatten if necessary\n",
    "\t\t\toutputs = model(inputs).float().cpu().numpy()\n",
    "\t\t\tpredictions = (outputs >= 0.5).float().cpu().numpy()\n",
    "\t\t\tall_preds.extend(predictions)\n",
    "\t\t\tall_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "\tall_preds = np.array(all_preds).squeeze()\n",
    "\tprint(all_preds)\n",
    "\n",
    "\treturn {\n",
    "\t\t'all_predictions': all_preds,\n",
    "\t\t'all_labels': all_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_predictions_final(model, test_loader):\n",
    "\tmodel.eval()\n",
    "\tall_preds = []\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tfor inputs in test_loader:\n",
    "\t\t\t# Ensure inputs are in the correct shape\n",
    "\t\t\tinputs = inputs.view(inputs.size(0), -1)  # Flatten if necessary\n",
    "\t\t\toutputs = model(inputs).float().cpu().numpy()\n",
    "\t\t\tpredictions = (outputs >= 0.5).float().cpu().numpy()\n",
    "\t\t\tall_preds.extend(predictions)\n",
    "\n",
    "\tall_preds = np.array(all_preds).squeeze()\n",
    "\tprint(all_preds)\n",
    "\n",
    "\treturn {\n",
    "\t\t'all_predictions': all_preds\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_predictions_prob(model, test_loader):\n",
    "\tmodel.eval()\n",
    "\tall_preds = []\n",
    "\tall_labels = []\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tfor inputs, labels in test_loader:\n",
    "\t\t\t# Ensure inputs are in the correct shape\n",
    "\t\t\tinputs = inputs.view(inputs.size(0), -1)  # Flatten if necessary\n",
    "\t\t\toutputs = model(inputs).float().cpu().numpy()\n",
    "\t\t\tall_preds.extend(outputs)\n",
    "\t\t\tall_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "\tall_preds = np.array(all_preds).squeeze()\n",
    "\tprint(all_preds)\n",
    "\n",
    "\treturn {\n",
    "\t\t'all_predictions': all_preds,\n",
    "\t\t'all_labels': all_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(X, y, best_params, dim, k=5):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    results = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        train_loader = create_dataloaders(X_train, y_train, best_params['batch_size'])\n",
    "        val_loader = create_dataloaders(X_test, y_test, best_params['batch_size'])\n",
    "\n",
    "        model = DynamicNeuralNetwork(dim, best_params['hidden_sizes'], best_params['dropout_rate']).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n",
    "        loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "        trained_model, _, _, _, _, _ = train_model(model, train_loader, val_loader, loss_fn, optimizer, 100, device)\n",
    "        results.append(evaluate_model(trained_model, dim, val_loader, False))\n",
    "\n",
    "    # Calculate metrics across all folds\n",
    "    metric_names = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    avg_metrics = {\n",
    "        metric: np.mean([result[metric] for result in results])\n",
    "        for metric in metric_names\n",
    "    }\n",
    "    \n",
    "    std_metrics = {\n",
    "        metric: np.std([result[metric] for result in results])\n",
    "        for metric in metric_names\n",
    "    }\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nAverage Results Across All Folds:\")\n",
    "    for metric in metric_names:\n",
    "        print(f\"{metric.capitalize()}: {avg_metrics[metric]:.4f} ± {std_metrics[metric]:.4f}\")\n",
    "    \n",
    "    return avg_metrics, std_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def objective(trial, dim):\n",
    "\tX_train = np.load(f'Datasets_Train_Test_Split/kryptonite-{dim}-X_train.npy')\n",
    "\ty_train = np.load(f'Datasets_Train_Test_Split/kryptonite-{dim}-y_train.npy')\n",
    "\n",
    "\tX_train = denoise_bernoulli_data(X_train)\n",
    "\n",
    "\tX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\tbatch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "\tlearning_rate = trial.suggest_float('learning_rate', 0.0001, 0.01, step=0.001)\n",
    "\tweight_decay = 0.0001\n",
    "\tdropout_rate = 0.15\n",
    "\tnum_layers = trial.suggest_int('num_layers', 2, 4)\n",
    "\thidden_sizes = [trial.suggest_categorical(f'hidden_size_l{i}', [64, 128, 256]) for i in range(num_layers)]\n",
    "\tnum_epochs = 200\n",
    "\n",
    "\ttrain_loader = create_dataloaders(X_train, y_train, batch_size)\n",
    "\tval_loader = create_dataloaders(X_val, y_val, batch_size)\n",
    "\tmodel = DynamicNeuralNetwork(dim, hidden_sizes=hidden_sizes, dropout_rate=dropout_rate).to(device)\n",
    "\toptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\tloss_fn = torch.nn.BCELoss()\n",
    "\t_, _, _, val_losses, _, _ = train_model(model, train_loader, val_loader, loss_fn, optimizer, num_epochs, device)\n",
    "\n",
    "\treturn val_losses[-1]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "best_params_by_dim = []\n",
    "\n",
    "for dim in dimensionality:\n",
    "\tprint(f\"-=-=-=-= Running Dimension: {dim} =-=-=-=-\")\n",
    "\tstudy = optuna.create_study(direction='minimize')\n",
    "\tstudy.optimize(lambda trial: objective(trial, dim), n_trials=10)\n",
    "\n",
    "\tprint(\"Best trial:\")\n",
    "\ttrial = study.best_trial\n",
    "\n",
    "\tprint(f\"  Value: {trial.value}\")\n",
    "\tprint(\"  Params: \")\n",
    "\tfor key, value in trial.params.items():\n",
    "\t\tprint(f\"    {key}: {value}\")\n",
    "\n",
    "\tbest_params = trial.params\n",
    "\tbest_params['hidden_sizes'] = [best_params.pop(f'hidden_size_l{i}') for i in range(best_params['num_layers'])]\n",
    "\tdel best_params['num_layers']\n",
    "\tbest_params_by_dim.append(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_true, y_pred_proba):\n",
    "    \"\"\"\n",
    "    Plot ROC curve and calculate AUC score from predictions and true values.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True binary labels (0, 1)\n",
    "    y_pred_proba : array-like\n",
    "        Predicted probabilities for the positive class\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        AUC score\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "             label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    plt.fill_between(fpr, tpr, alpha=0.2, color='darkorange')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_multi_roc_curves(y_true_list, y_pred_proba_list, dimensions):\n",
    "    \"\"\"\n",
    "    Plot ROC curves for multiple dimensions on the same figure.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true_list : list of array-like\n",
    "        List of true binary labels for each dimension\n",
    "    y_pred_proba_list : list of array-like\n",
    "        List of predicted probabilities for each dimension\n",
    "    dimensions : list\n",
    "        List of dimension values\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary of AUC scores for each dimension\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    auc_scores = {}\n",
    "    \n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(dimensions)))\n",
    "    \n",
    "    for (y_true, y_pred_proba, dim, color) in zip(y_true_list, y_pred_proba_list, dimensions, colors):\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        auc_scores[dim] = roc_auc\n",
    "        \n",
    "        plt.plot(fpr, tpr, color=color, lw=2, \n",
    "                label=f'Dim {dim} (AUC = {roc_auc:.2f})')\n",
    "        plt.fill_between(fpr, tpr, alpha=0.1, color=color)\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=22)\n",
    "    plt.ylabel('True Positive Rate', fontsize=22)\n",
    "    plt.title('ROC Curves for Different Dimensions', fontsize=22)\n",
    "    plt.legend(loc=\"lower right\", bbox_to_anchor=(1.15, 0), fontsize=22)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    return auc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_true_list = []\n",
    "y_pred_proba_list = []\n",
    "\n",
    "for i, dim in enumerate(dimensionality):\n",
    "\tprint(f\"-=-=-=-= Running {dim} =-=-=-=-\")\n",
    "\tX_train = np.load(r'Datasets_Train_Test_Split/kryptonite-{dim}-X_train.npy')\n",
    "\tX_test = np.load(r'Datasets_Train_Test_Split/kryptonite-{dim}-X_test.npy')\n",
    "\ty_train = np.load(r'Datasets_Train_Test_Split/kryptonite-{dim}-y_train.npy')\n",
    "\ty_test = np.load(r'Datasets_Train_Test_Split/kryptonite-{dim}-y_test.npy')\n",
    "\n",
    "\tX_train = denoise_bernoulli_data(X_train)\n",
    "\tX_test = denoise_bernoulli_data(X_test)\n",
    "\n",
    "\tX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\tbest_params = best_params_by_dim[i]\n",
    "\ttrain_loader = create_dataloaders(X_train, y_train, best_params['batch_size'])\n",
    "\tval_loader = create_dataloaders(X_val, y_val, best_params['batch_size'])\n",
    "\ttest_loader = create_dataloaders(X_test, y_test, best_params['batch_size'])\n",
    "\tmodel = DynamicNeuralNetwork(dim, best_params['hidden_sizes'], 0.15).to(device)\n",
    "\toptimizer = torch.optim.Adam(model.parameters(), lr=best_params['learning_rate'], weight_decay=0.0001)\n",
    "\tloss_fn = torch.nn.BCELoss()\n",
    "\ttrained_model, _, _, _, _, _ = train_model(model, train_loader, val_loader, loss_fn, optimizer, 200, device)\n",
    "\ttest_metrics_run = evaluate_model(trained_model, dim, test_loader, False)\n",
    "\tfor metric, value in test_metrics_run.items():\n",
    "\t\tprint(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "\tpredictions = get_predictions_prob(trained_model, test_loader)\n",
    "\ty_true_list.append(predictions['all_labels'])\n",
    "\ty_pred_proba_list.append(predictions['all_predictions'])\n",
    "\n",
    "\tplot_roc_curve(predictions['all_labels'], predictions['all_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_multi_roc_curves(y_true_list, y_pred_proba_list, dimensionality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, dim in enumerate(dimensionality):\n",
    "\tprint(f\"-=-=-=-= Running {dim} =-=-=-=-\")\n",
    "\n",
    "\tbest_params = best_params_by_dim[i]\n",
    "\n",
    "\tX = np.load(f'Datasets_Train_Test_Split/kryptonite-{dim}-X.npy')\n",
    "\ty = np.load(f'Datasets_Train_Test_Split/kryptonite-{dim}-y.npy')\n",
    "\n",
    "\tX = denoise_bernoulli_data(X)\n",
    "\n",
    "\t# Get both metrics and standard deviations\n",
    "\tavg_metrics, std_metrics = k_fold_cross_validation(X, y, best_params, dim, k=2)\n",
    "\n",
    "\t# Print results with proper formatting and ± symbol\n",
    "\tprint(\"\\nFinal K-Fold Cross Validation Results:\")\n",
    "\tprint(f\"Average Accuracy:  {avg_metrics['accuracy']:.4f} ± {std_metrics['accuracy']:.4f}\")\n",
    "\tprint(f\"Average Precision: {avg_metrics['precision']:.4f} ± {std_metrics['precision']:.4f}\")\n",
    "\tprint(f\"Average Recall:    {avg_metrics['recall']:.4f} ± {std_metrics['recall']:.4f}\")\n",
    "\tprint(f\"Average F1:        {avg_metrics['f1']:.4f} ± {std_metrics['f1']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MML-CW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
