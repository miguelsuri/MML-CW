{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7042e316ad91f3c2",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93d8004fea2451bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c553d85ba3a7e13b",
   "metadata": {},
   "source": [
    "We create the data loaders of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adc548e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_bernoulli_data(X):\n",
    "    \"\"\"Denoise data by converting to binary values\"\"\"\n",
    "    # return X\n",
    "    return (X >= 0.5).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac10185037ae0c71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T12:06:20.658063Z",
     "start_time": "2024-11-05T12:06:20.651706Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataloaders(dimensionality, batch_size=32, random_state=42):\n",
    "    \"\"\"\n",
    "    Create PyTorch DataLoaders for training, validation, and test sets.\n",
    "    \n",
    "    Args:\n",
    "        dimensionality (int): Dimensionality of the input features\n",
    "        batch_size (int): Batch size for DataLoaders\n",
    "        random_state (int): Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_loader, val_loader, test_loader)\n",
    "    \"\"\"\n",
    "\n",
    "    # Load pre-defined train and test datasets\n",
    "    X_train = np.load(f'../../Datasets_Train_Test_Split/kryptonite-{dimensionality}-X_train.npy')\n",
    "    y_train = np.load(f'../../Datasets_Train_Test_Split/kryptonite-{dimensionality}-y_train.npy')\n",
    "    X_test = np.load(f'../../Datasets_Train_Test_Split/kryptonite-{dimensionality}-X_test.npy')\n",
    "    y_test = np.load(f'../../Datasets_Train_Test_Split/kryptonite-{dimensionality}-y_test.npy')\n",
    "    \n",
    "    # Split the training data into new training data (80%) and validation data (20%)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train,\n",
    "        test_size=0.2,  # 20% for validation\n",
    "        random_state=random_state,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Apply denoising\n",
    "    X_train = denoise_bernoulli_data(X_train)\n",
    "    X_val = denoise_bernoulli_data(X_val)\n",
    "    X_test = denoise_bernoulli_data(X_test)\n",
    "\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "    \n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f\"Dataset splits:\")\n",
    "    print(f\"Training set size: {len(X_train)}\")\n",
    "    print(f\"Validation set size: {len(X_val)}\")\n",
    "    print(f\"Test set size: {len(X_test)}\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523b730dad6f795f",
   "metadata": {},
   "source": [
    "We now create the Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b1c09a76f33417f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T12:06:20.706306Z",
     "start_time": "2024-11-05T12:06:20.701310Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For Binary Classification:\n",
    "1. Input layer shape has the same number of neurons as the number of features in the dataset\n",
    "2. At least 1 hidden layer\n",
    "3. 10 to 512 neurons per hidden layer\n",
    "4. The output layer shape has 1 neuron (one class or the other)\n",
    "5. Typically we use ReLU as hidden layer activation function\n",
    "6. We use Sigmoid as the output layer activation function for binary classification (torch.sigmoid)\n",
    "7. We use Binary Cross Entropy as the loss function (nn.BCELoss)\n",
    "8. The optimizer can be SGD, Adam, etc.\n",
    "\"\"\"\n",
    "\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_dim=45, dropout_rate=0.3):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        # Ensure input_dim is correct\n",
    "        self.network = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, 128),  # Input shape: (batch_size, input_dim)\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout_rate),\n",
    "            \n",
    "            torch.nn.Linear(128, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout_rate),\n",
    "            \n",
    "            torch.nn.Linear(128, 1),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "        # Initialize weights, based on https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/Basics/pytorch_init_weights.py\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                torch.nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "                torch.nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Ensure input is in the correct shape\n",
    "        # x shape should be (batch_size, input_dim)\n",
    "        if len(x.shape) == 1:\n",
    "            x = x.unsqueeze(0)  # Add batch dimension if missing\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0cb9d0ad63ce85",
   "metadata": {},
   "source": [
    "We now create the methods to train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a396626ce69fb2ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T12:06:20.760832Z",
     "start_time": "2024-11-05T12:06:20.749188Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, loss_fn, optimizer, num_epochs=1000, early_stopping_patience=50):\n",
    "    # Accuracies are computed based on https://saturncloud.io/blog/calculating-the-accuracy-of-pytorch-models-every-epoch/\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            dim_size = inputs.size(1)\n",
    "\n",
    "            # Ensure inputs are in the correct shape\n",
    "            inputs = inputs.view(inputs.size(0), -1)  # Flatten if necessary\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels.unsqueeze(1).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predicted = (outputs >= 0.5).float()\n",
    "            train_correct += (predicted == labels.unsqueeze(1)).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_accuracy = train_correct / train_total * 100\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                # Ensure inputs are in the correct shape\n",
    "                inputs = inputs.view(inputs.size(0), -1)  # Flatten if necessary\n",
    "\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                loss = loss_fn(outputs, labels.unsqueeze(1).float())\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                predicted = (outputs >= 0.5).float()\n",
    "                val_correct += (predicted == labels.unsqueeze(1)).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "\n",
    "            save_model(model, f'best_model_{dim_size}.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(f'Early stopping at epoch {epoch + 1}')\n",
    "            break\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader, dim):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            # Ensure inputs are in the correct shape\n",
    "            inputs = inputs.view(inputs.size(0), -1)  # Flatten if necessary\n",
    "            outputs = model(inputs)\n",
    "            predictions = (outputs >= 0.5).float().cpu().numpy()\n",
    "            all_preds.extend(predictions)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds).squeeze()\n",
    "    print(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    np.save(f'kryptonite_{dim}_pred_nn.npy', all_preds)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy_score(all_labels, all_preds),\n",
    "        'precision': precision_score(all_labels, all_preds),\n",
    "        'recall': recall_score(all_labels, all_preds),\n",
    "        'f1': f1_score(all_labels, all_preds)\n",
    "    }\n",
    "\n",
    "# Save the model\n",
    "def save_model(model, path='model.pth'):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "# Load the model\n",
    "def load_model_weights(model, path='model.pth'):\n",
    "    model.load_state_dict(torch.load(path, weights_only=True))\n",
    "    \n",
    "def load_model(path='model.pth'):\n",
    "    return torch.load(path, weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e29d677859c15f4",
   "metadata": {},
   "source": [
    "Finally, the method to actually train our model and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a6a1d68cc5612e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T12:06:20.811309Z",
     "start_time": "2024-11-05T12:06:20.804510Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_learning_curves(train_losses, val_losses, train_accuracies, val_accuracies):\n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Curves')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    plt.plot(train_accuracies, label='Training Accuracy')\n",
    "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "    \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy Curves')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def run_training(train_loader, val_loader, device, input_dim=45, num_epochs=1000, early_stopping_patience=50, learning_rate=0.001, weight_decay=0.001, momentum=0.9, dropout_rate=0.3, test_loader=None):\n",
    "\t# Initialize model and move to device\n",
    "\tmodel = NeuralNetwork(input_dim=input_dim, dropout_rate=dropout_rate).to(device)\n",
    "\t# We use binary cross-entropy, for Binary classification. We apply Sigmoid in the NN\n",
    "\tcriterion = torch.nn.BCELoss()\n",
    "\t# We use Stochastic Gradient Descent with L2 regularization and momentum\n",
    "\t# Momentum: https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d\n",
    "\t# L2 Reg: https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with-weight-regularization/ \n",
    "\toptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay, amsgrad=True)\n",
    "\t# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=momentum)\n",
    "\tprint(f\"\\nTraining-------------------------------\")\n",
    "\t# Train the model\n",
    "\ttrain_losses, val_losses, train_accuracies, val_accuracies = train_model(\n",
    "\t    model=model,\n",
    "\t    train_loader=train_loader,\n",
    "\t    val_loader=val_loader,\n",
    "\t    loss_fn=criterion,\n",
    "\t    optimizer=optimizer,\n",
    "\t    num_epochs=num_epochs,\n",
    "\t    early_stopping_patience=early_stopping_patience\n",
    "\t)\n",
    "\t# Load the best model for evaluation\n",
    "\tload_model_weights(model, f'best_model_{input_dim}.pth')\n",
    "\tprint(f\"\\nEvaluating-------------------------------\")\n",
    "      \n",
    "\ttest_metrics = {}\n",
    "    \n",
    "\tplot_learning_curves(train_losses, val_losses, train_accuracies, val_accuracies)\n",
    "\treturn model, test_metrics, train_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c90562908597520",
   "metadata": {},
   "source": [
    "We can now run the training and evaluation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd5f6d97e29dc7d",
   "metadata": {},
   "source": [
    "We set the dimensionality of the dataset first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d08fd3623a3e831",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T12:06:20.857403Z",
     "start_time": "2024-11-05T12:06:20.854448Z"
    }
   },
   "outputs": [],
   "source": [
    "momentum = 0.9\n",
    "weight_decay = 0.0001\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "early_stopping_patience = 50\n",
    "dropout_rate = 0.15\n",
    "batch_size=16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec268ca1c50409f",
   "metadata": {},
   "source": [
    "And we get the device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db1a3682cdddc69a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T12:06:20.907023Z",
     "start_time": "2024-11-05T12:06:20.904339Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484c4b507cc4c792",
   "metadata": {},
   "source": [
    "Finally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b543773a0839812",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T12:16:56.887090Z",
     "start_time": "2024-11-05T12:06:21.011405Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_one_dim(dimension: int, save: bool):\n",
    "\tprint(f\"Running code for dim {dimension}\")\n",
    "\n",
    "\ttrain_loader, val_loader, test_loader = create_dataloaders(dimensionality=dimension, batch_size=batch_size)\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\t# Assuming you have already created your DataLoaders\n",
    "\tmodel, test_metrics_run, train_accuracies_run = run_training(\n",
    "\t    train_loader, val_loader, test_loader=test_loader, input_dim=dimension, device=device, learning_rate=learning_rate,\n",
    "\t    weight_decay=weight_decay, momentum=momentum, num_epochs=num_epochs, early_stopping_patience=early_stopping_patience,\n",
    "\t    dropout_rate=dropout_rate)\t\n",
    "\tend_time = time.time()\n",
    "\n",
    "\tprint(f\"\\nTraining took: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    # Dictionary to store both metrics together\n",
    "\tmetrics_dict = {\n",
    "        'training_accuracies': train_accuracies_run,\n",
    "\t\t'test_accuracy': test_metrics_run.get('accuracy')\n",
    "\t}\n",
    "\n",
    "\t# Save the dictionary as an npy file\n",
    "\tif save:\n",
    "\t\tnp.save(f'metrics_dim_{dimension}.npy', metrics_dict)\n",
    "\n",
    "\tprint(\"\\nTest Set Metrics:\")\n",
    "\tfor metric, value in test_metrics_run.items():\n",
    "\t    print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "def run_all_dims(dimensions, save_files):\n",
    "\n",
    "\tfor dimension in dimensions:\n",
    "\t\trun_one_dim(dimension, save_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08cc70d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running code for dim 9\n",
      "Dataset splits:\n",
      "Training set size: 11520\n",
      "Validation set size: 2880\n",
      "Test set size: 3600\n",
      "\n",
      "Training-------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m dimensions \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m18\u001b[39m]  \u001b[38;5;66;03m# List of dimensions\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m run_all_dims(dimensions, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[19], line 33\u001b[0m, in \u001b[0;36mrun_all_dims\u001b[1;34m(dimensions, save_files)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_all_dims\u001b[39m(dimensions, save_files):\n\u001b[0;32m     32\u001b[0m \t\u001b[38;5;28;01mfor\u001b[39;00m dimension \u001b[38;5;129;01min\u001b[39;00m dimensions:\n\u001b[1;32m---> 33\u001b[0m \t\trun_one_dim(dimension, save_files)\n",
      "Cell \u001b[1;32mIn[19], line 8\u001b[0m, in \u001b[0;36mrun_one_dim\u001b[1;34m(dimension, save)\u001b[0m\n\u001b[0;32m      5\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Assuming you have already created your DataLoaders\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m model, test_metrics_run, train_accuracies_run \u001b[38;5;241m=\u001b[39m run_training(\n\u001b[0;32m      9\u001b[0m     train_loader, val_loader, test_loader\u001b[38;5;241m=\u001b[39mtest_loader, input_dim\u001b[38;5;241m=\u001b[39mdimension, device\u001b[38;5;241m=\u001b[39mdevice, learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate,\n\u001b[0;32m     10\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mweight_decay, momentum\u001b[38;5;241m=\u001b[39mmomentum, num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs, early_stopping_patience\u001b[38;5;241m=\u001b[39mearly_stopping_patience,\n\u001b[0;32m     11\u001b[0m     dropout_rate\u001b[38;5;241m=\u001b[39mdropout_rate)\t\n\u001b[0;32m     12\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining took: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 37\u001b[0m, in \u001b[0;36mrun_training\u001b[1;34m(train_loader, val_loader, device, input_dim, num_epochs, early_stopping_patience, learning_rate, weight_decay, momentum, dropout_rate, test_loader)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m train_losses, val_losses, train_accuracies, val_accuracies \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[0;32m     38\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     39\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[0;32m     40\u001b[0m     val_loader\u001b[38;5;241m=\u001b[39mval_loader,\n\u001b[0;32m     41\u001b[0m     loss_fn\u001b[38;5;241m=\u001b[39mcriterion,\n\u001b[0;32m     42\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m     43\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs,\n\u001b[0;32m     44\u001b[0m     early_stopping_patience\u001b[38;5;241m=\u001b[39mearly_stopping_patience\n\u001b[0;32m     45\u001b[0m )\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Load the best model for evaluation\u001b[39;00m\n\u001b[0;32m     47\u001b[0m load_model_weights(model, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 27\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, loss_fn, optimizer, num_epochs, early_stopping_patience)\u001b[0m\n\u001b[0;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m     26\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     29\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\migue\\anaconda3\\envs\\MML-CW\\Lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\migue\\anaconda3\\envs\\MML-CW\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\migue\\anaconda3\\envs\\MML-CW\\Lib\\site-packages\\torch\\optim\\adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    214\u001b[0m         group,\n\u001b[0;32m    215\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m         state_steps,\n\u001b[0;32m    221\u001b[0m     )\n\u001b[1;32m--> 223\u001b[0m     adam(\n\u001b[0;32m    224\u001b[0m         params_with_grad,\n\u001b[0;32m    225\u001b[0m         grads,\n\u001b[0;32m    226\u001b[0m         exp_avgs,\n\u001b[0;32m    227\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    228\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    229\u001b[0m         state_steps,\n\u001b[0;32m    230\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    231\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    232\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    233\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    234\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    235\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    236\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    237\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    238\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    239\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    240\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    241\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    242\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    243\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    244\u001b[0m     )\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\migue\\anaconda3\\envs\\MML-CW\\Lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\migue\\anaconda3\\envs\\MML-CW\\Lib\\site-packages\\torch\\optim\\adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 784\u001b[0m func(\n\u001b[0;32m    785\u001b[0m     params,\n\u001b[0;32m    786\u001b[0m     grads,\n\u001b[0;32m    787\u001b[0m     exp_avgs,\n\u001b[0;32m    788\u001b[0m     exp_avg_sqs,\n\u001b[0;32m    789\u001b[0m     max_exp_avg_sqs,\n\u001b[0;32m    790\u001b[0m     state_steps,\n\u001b[0;32m    791\u001b[0m     amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[0;32m    792\u001b[0m     has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    793\u001b[0m     beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    794\u001b[0m     beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    795\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m    796\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[0;32m    797\u001b[0m     eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m    798\u001b[0m     maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[0;32m    799\u001b[0m     capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[0;32m    800\u001b[0m     differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[0;32m    801\u001b[0m     grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[0;32m    802\u001b[0m     found_inf\u001b[38;5;241m=\u001b[39mfound_inf,\n\u001b[0;32m    803\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\migue\\anaconda3\\envs\\MML-CW\\Lib\\site-packages\\torch\\optim\\adam.py:524\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;66;03m# Update steps\u001b[39;00m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;66;03m# If steps are on CPU, foreach will fall back to the slow path, which is a for-loop calling t.add(1) over\u001b[39;00m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;66;03m# and over. 1 will then be wrapped into a Tensor over and over again, which is slower than if we just\u001b[39;00m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;66;03m# wrapped it once now. The alpha is required to assure we go to the right overload.\u001b[39;00m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39mis_compiling() \u001b[38;5;129;01mand\u001b[39;00m device_state_steps[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mis_cpu:\n\u001b[1;32m--> 524\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_add_(\n\u001b[0;32m    525\u001b[0m         device_state_steps, torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m1.0\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m), alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m    526\u001b[0m     )\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    528\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_add_(device_state_steps, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dimensions = [9, 12, 15, 18]  # List of dimensions\n",
    "\n",
    "run_all_dims(dimensions, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MML-CW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
