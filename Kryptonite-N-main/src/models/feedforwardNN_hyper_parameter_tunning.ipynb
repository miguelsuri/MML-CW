{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import of the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We run the code in GPU using cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "dimensionality = [9, 12, 15, 18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DynamicNeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, dim, hidden_sizes, dropout_rate):\n",
    "        super(DynamicNeuralNetwork, self).__init__()\n",
    "        layers = []\n",
    "        last_size = dim\n",
    "\n",
    "        for size in hidden_sizes:\n",
    "            layers.append(torch.nn.Linear(last_size, size))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            layers.append(torch.nn.Dropout(dropout_rate))\n",
    "            last_size = size\n",
    "\n",
    "        layers.append(torch.nn.Linear(last_size, 1))\n",
    "        layers.append(torch.nn.Sigmoid())\n",
    "        self.network = torch.nn.Sequential(*layers)\n",
    "\n",
    "        # Initialize weights, based on https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/Basics/pytorch_init_weights.py\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                torch.nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "                torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing and data loaders defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def denoise_bernoulli_data(X):\n",
    "    \"\"\"Denoise data by converting to binary values\"\"\"\n",
    "    return (X >= 0.5).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_dataloaders(X, y, batch_size):\n",
    "    \"\"\"Convert numpy arrays to PyTorch tensors and create DataLoaders.\"\"\"\n",
    "    X = denoise_bernoulli_data(X)\n",
    "    X = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "    y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "\n",
    "    dataset = TensorDataset(X, y)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function that spcifies how the model is trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, loss_fn, optimizer, num_epochs, device, early_stopping_patience=50, print_statements=False):\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "        # Train Model\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels.unsqueeze(1).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predicted = (outputs >= 0.5).float()\n",
    "            train_correct += (predicted == labels.unsqueeze(1)).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_accuracy = train_correct / train_total * 100\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Evaluate the model\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels.unsqueeze(1).float())\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                predicted = (outputs >= 0.5).float()\n",
    "                val_correct += (predicted == labels.unsqueeze(1)).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        \n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "\n",
    "        if patience_counter >= early_stopping_patience and print_statements:\n",
    "            print(f'Early stopping at epoch {epoch + 1}')\n",
    "            break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0 and print_statements:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    return model, best_val_loss, train_losses, val_losses, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the trained model using test loader definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, dim, test_loader, save_file):\n",
    "\tmodel.eval()\n",
    "\tall_preds = []\n",
    "\tall_labels = []\n",
    "    \n",
    "\twith torch.no_grad():\n",
    "\t\tfor inputs, labels in test_loader:\n",
    "\t\t\tinputs = inputs.view(inputs.size(0), -1)\n",
    "\t\t\toutputs = model(inputs)\n",
    "\t\t\tpredictions = (outputs >= 0.5).float().cpu().numpy()\n",
    "\t\t\tall_preds.extend(predictions)\n",
    "\t\t\tall_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "\tall_preds = np.array(all_preds).squeeze()\n",
    "\tprint(all_preds)\n",
    "\t\n",
    "\tif save_file:    \n",
    "\t\tall_labels = np.array(all_labels)\n",
    "\t\tnp.save(f'kryptonite_{dim}_pred_nn.npy', all_preds)\n",
    "\n",
    "\treturn {\n",
    "        'accuracy': accuracy_score(all_labels, all_preds),\n",
    "        'precision': precision_score(all_labels, all_preds),\n",
    "        'recall': recall_score(all_labels, all_preds),\n",
    "        'f1': f1_score(all_labels, all_preds)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to get predictions\n",
    "\n",
    "- get_predictions is mostly used for the model evaluation.\n",
    "- get_predictions_final is used for the predictions on the hidden data.\n",
    "- get_predictions_prob is used to get the probability for the sigmoid link function. This is used for the AUC graph and ROC scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, test_loader):\n",
    "\tmodel.eval()\n",
    "\tall_preds = []\n",
    "\tall_labels = []\n",
    "    \n",
    "\twith torch.no_grad():\n",
    "\t\tfor inputs, labels in test_loader:\n",
    "\t\t\tinputs = inputs.view(inputs.size(0), -1)\n",
    "\t\t\toutputs = model(inputs).float().cpu().numpy()\n",
    "\t\t\tpredictions = (outputs >= 0.5).float().cpu().numpy()\n",
    "\t\t\tall_preds.extend(predictions)\n",
    "\t\t\tall_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "\tall_preds = np.array(all_preds).squeeze()\n",
    "\tprint(all_preds)\n",
    "\n",
    "\treturn {\n",
    "\t\t'all_predictions': all_preds,\n",
    "\t\t'all_labels': all_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_predictions_final(model, test_loader):\n",
    "\tmodel.eval()\n",
    "\tall_preds = []\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tfor inputs in test_loader:\n",
    "\t\t\tinputs = inputs.view(inputs.size(0), -1)\n",
    "\t\t\toutputs = model(inputs).float().cpu().numpy()\n",
    "\t\t\tpredictions = (outputs >= 0.5).float().cpu().numpy()\n",
    "\t\t\tall_preds.extend(predictions)\n",
    "\n",
    "\tall_preds = np.array(all_preds).squeeze()\n",
    "\tprint(all_preds)\n",
    "\n",
    "\treturn {\n",
    "\t\t'all_predictions': all_preds\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_predictions_prob(model, test_loader):\n",
    "\tmodel.eval()\n",
    "\tall_preds = []\n",
    "\tall_labels = []\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tfor inputs, labels in test_loader:\n",
    "\t\t\tinputs = inputs.view(inputs.size(0), -1)\n",
    "\t\t\toutputs = model(inputs).float().cpu().numpy()\n",
    "\t\t\tall_preds.extend(outputs)\n",
    "\t\t\tall_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "\tall_preds = np.array(all_preds).squeeze()\n",
    "\tprint(all_preds)\n",
    "\n",
    "\treturn {\n",
    "\t\t'all_predictions': all_preds,\n",
    "\t\t'all_labels': all_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defintion of cross validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(X, y, best_params, dim, k=5):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    results = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        train_loader = create_dataloaders(X_train, y_train, best_params['batch_size'])\n",
    "        val_loader = create_dataloaders(X_test, y_test, best_params['batch_size'])\n",
    "\n",
    "        model = DynamicNeuralNetwork(dim, best_params['hidden_sizes'], best_params['dropout_rate']).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n",
    "        loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "        trained_model, _, _, _, _, _ = train_model(model, train_loader, val_loader, loss_fn, optimizer, 100, device)\n",
    "        results.append(evaluate_model(trained_model, dim, val_loader, False))\n",
    "\n",
    "    metric_names = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    avg_metrics = {\n",
    "        metric: np.mean([result[metric] for result in results])\n",
    "        for metric in metric_names\n",
    "    }\n",
    "    \n",
    "    std_metrics = {\n",
    "        metric: np.std([result[metric] for result in results])\n",
    "        for metric in metric_names\n",
    "    }\n",
    "    \n",
    "    print(\"\\nAverage Results Across All Folds:\")\n",
    "    for metric in metric_names:\n",
    "        print(f\"{metric.capitalize()}: {avg_metrics[metric]:.4f} ± {std_metrics[metric]:.4f}\")\n",
    "    \n",
    "    return avg_metrics, std_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defintion of the objective function we minize in the Optuna framework\n",
    "\n",
    "This function is used to find the best hyper paramters (it performs the hyper parameter search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def objective(trial, dim):\n",
    "\tX_train = np.load(f'../../Datasets_Train_Test_Split/kryptonite-{dim}-X_train.npy')\n",
    "\ty_train = np.load(f'../../Datasets_Train_Test_Split/kryptonite-{dim}-y_train.npy')\n",
    "\n",
    "\tX_train = denoise_bernoulli_data(X_train)\n",
    "\n",
    "\tX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\tbatch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "\tlearning_rate = trial.suggest_float('learning_rate', 0.0001, 0.01, step=0.001)\n",
    "\tweight_decay = 0.0001\n",
    "\tdropout_rate = 0.15\n",
    "\tnum_layers = trial.suggest_int('num_layers', 2, 4)\n",
    "\thidden_sizes = [trial.suggest_categorical(f'hidden_size_l{i}', [64, 128, 256]) for i in range(num_layers)]\n",
    "\tnum_epochs = 200\n",
    "\n",
    "\ttrain_loader = create_dataloaders(X_train, y_train, batch_size)\n",
    "\tval_loader = create_dataloaders(X_val, y_val, batch_size)\n",
    "\tmodel = DynamicNeuralNetwork(dim, hidden_sizes=hidden_sizes, dropout_rate=dropout_rate).to(device)\n",
    "\toptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\tloss_fn = torch.nn.BCELoss()\n",
    "\t_, _, _, val_losses, _, _ = train_model(model, train_loader, val_loader, loss_fn, optimizer, num_epochs, device)\n",
    "\n",
    "\treturn val_losses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-25 17:49:52,325] A new study created in memory with name: no-name-e608ba07-6760-46df-b01f-fd3aedb383c6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-=-=-=-= Running Dimension: 9 =-=-=-=-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\migue\\anaconda3\\envs\\MML-CW\\Lib\\site-packages\\optuna\\distributions.py:693: UserWarning: The distribution is specified by [0.0001, 0.01] and step=0.001, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.0091].\n",
      "  warnings.warn(\n",
      "  2%|▏         | 3/200 [00:06<07:31,  2.29s/it]\n",
      "[W 2024-11-25 17:50:01,636] Trial 0 failed with parameters: {'batch_size': 16, 'learning_rate': 0.0001, 'num_layers': 3, 'hidden_size_l0': 64, 'hidden_size_l1': 256, 'hidden_size_l2': 128} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\migue\\anaconda3\\envs\\MML-CW\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\migue\\AppData\\Local\\Temp\\ipykernel_20368\\3240112142.py\", line 7, in <lambda>\n",
      "    study.optimize(lambda trial: objective(trial, dim), n_trials=10)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\migue\\AppData\\Local\\Temp\\ipykernel_20368\\501248735.py\", line 22, in objective\n",
      "    _, _, _, val_losses, _, _ = train_model(model, train_loader, val_loader, loss_fn, optimizer, num_epochs, device)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\migue\\AppData\\Local\\Temp\\ipykernel_20368\\3213163761.py\", line 22, in train_model\n",
      "    loss.backward()\n",
      "  File \"c:\\Users\\migue\\anaconda3\\envs\\MML-CW\\Lib\\site-packages\\torch\\_tensor.py\", line 581, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"c:\\Users\\migue\\anaconda3\\envs\\MML-CW\\Lib\\site-packages\\torch\\autograd\\__init__.py\", line 347, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"c:\\Users\\migue\\anaconda3\\envs\\MML-CW\\Lib\\site-packages\\torch\\autograd\\graph.py\", line 825, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2024-11-25 17:50:01,641] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-=-=-=-= Running Dimension: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m =-=-=-=-\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: objective(trial, dim), n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m trial \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\n",
      "File \u001b[1;32mc:\\Users\\migue\\anaconda3\\envs\\MML-CW\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     _optimize(\n\u001b[0;32m    476\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    477\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m    478\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[0;32m    479\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    480\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    481\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[0;32m    482\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    483\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[0;32m    484\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[0;32m    485\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\migue\\anaconda3\\envs\\MML-CW\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         _optimize_sequential(\n\u001b[0;32m     64\u001b[0m             study,\n\u001b[0;32m     65\u001b[0m             func,\n\u001b[0;32m     66\u001b[0m             n_trials,\n\u001b[0;32m     67\u001b[0m             timeout,\n\u001b[0;32m     68\u001b[0m             catch,\n\u001b[0;32m     69\u001b[0m             callbacks,\n\u001b[0;32m     70\u001b[0m             gc_after_trial,\n\u001b[0;32m     71\u001b[0m             reseed_sampler_rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     72\u001b[0m             time_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     73\u001b[0m             progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[0;32m     74\u001b[0m         )\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\migue\\anaconda3\\envs\\MML-CW\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\migue\\anaconda3\\envs\\MML-CW\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\migue\\anaconda3\\envs\\MML-CW\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m func(trial)\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[15], line 7\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-=-=-=-= Running Dimension: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m =-=-=-=-\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: objective(trial, dim), n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m trial \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\n",
      "Cell \u001b[1;32mIn[14], line 22\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial, dim)\u001b[0m\n\u001b[0;32m     20\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[0;32m     21\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[1;32m---> 22\u001b[0m _, _, _, val_losses, _, _ \u001b[38;5;241m=\u001b[39m train_model(model, train_loader, val_loader, loss_fn, optimizer, num_epochs, device)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m val_losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[1;32mIn[6], line 22\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, loss_fn, optimizer, num_epochs, device, early_stopping_patience, print_statements)\u001b[0m\n\u001b[0;32m     20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m---> 22\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     25\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\migue\\anaconda3\\envs\\MML-CW\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    583\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\migue\\anaconda3\\envs\\MML-CW\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\migue\\anaconda3\\envs\\MML-CW\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_params_by_dim = []\n",
    "\n",
    "for dim in dimensionality:\n",
    "\tprint(f\"-=-=-=-= Running Dimension: {dim} =-=-=-=-\")\n",
    "\n",
    "\tstudy = optuna.create_study(direction='minimize')\n",
    "\tstudy.optimize(lambda trial: objective(trial, dim), n_trials=10)\n",
    "\n",
    "\tprint(\"Best trial:\")\n",
    "\ttrial = study.best_trial\n",
    "\n",
    "\tprint(f\"  Value: {trial.value}\")\n",
    "\tprint(\"  Params: \")\n",
    "\n",
    "\tfor key, value in trial.params.items():\n",
    "\t\tprint(f\"    {key}: {value}\")\n",
    "\n",
    "\tbest_params = trial.params\n",
    "\tbest_params['hidden_sizes'] = [best_params.pop(f'hidden_size_l{i}') for i in range(best_params['num_layers'])]\n",
    "\n",
    "\tdel best_params['num_layers']\n",
    "\tbest_params_by_dim.append(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defintions of functions to plot the AUC and get ROC score\n",
    "\n",
    "- plot_roc_curve plots one graph for one dimesnion\n",
    "- plot_multi_roc_curves plots multiple AUCs for multiple dimensions in one graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_true, y_pred_proba):\n",
    "    \"\"\"\n",
    "    Plot ROC curve and calculate AUC score from predictions and true values.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True binary labels (0, 1)\n",
    "    y_pred_proba : array-like\n",
    "        Predicted probabilities for the positive class\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        AUC score\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "             label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    plt.fill_between(fpr, tpr, alpha=0.2, color='darkorange')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_multi_roc_curves(y_true_list, y_pred_proba_list, dimensions):\n",
    "    \"\"\"\n",
    "    Plot ROC curves for multiple dimensions on the same figure.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true_list : list of array-like\n",
    "        List of true binary labels for each dimension\n",
    "    y_pred_proba_list : list of array-like\n",
    "        List of predicted probabilities for each dimension\n",
    "    dimensions : list\n",
    "        List of dimension values\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary of AUC scores for each dimension\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    auc_scores = {}\n",
    "    \n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(dimensions)))\n",
    "    \n",
    "    for (y_true, y_pred_proba, dim, color) in zip(y_true_list, y_pred_proba_list, dimensions, colors):\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        auc_scores[dim] = roc_auc\n",
    "        \n",
    "        plt.plot(fpr, tpr, color=color, lw=2, \n",
    "                label=f'Dim {dim} (AUC = {roc_auc:.2f})')\n",
    "        plt.fill_between(fpr, tpr, alpha=0.1, color=color)\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "\n",
    "    plt.xlabel('False Positive Rate', fontsize=22)\n",
    "    plt.ylabel('True Positive Rate', fontsize=22)\n",
    "    plt.title('ROC Curves for Different Dimensions', fontsize=22)\n",
    "    plt.legend(loc=\"lower right\", bbox_to_anchor=(1.15, 0), fontsize=22)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    return auc_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to plot AUC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters = {\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 0.0001,\n",
    "    'dropout_rate': 0.15,\n",
    "    'num_layers': 4,  # Assuming max suggested layers since specific layer count not given\n",
    "    'hidden_sizes': [256, 256, 256, 256],  # Assuming max suggested size for each layer since specific sizes not given\n",
    "    'num_epochs': 1000,\n",
    "    'momentum': 0.9,\n",
    "    'early_stopping_patience': 50\n",
    "}\n",
    "\n",
    "best_params_by_dim = [best_parameters,best_parameters,best_parameters,best_parameters,best_parameters]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-=-=-=-= Running 9 =-=-=-=-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/200 [00:07<08:38,  2.63s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m], weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[0;32m     22\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[1;32m---> 23\u001b[0m trained_model, _, _, _, _, _ \u001b[38;5;241m=\u001b[39m train_model(model, train_loader, val_loader, loss_fn, optimizer, \u001b[38;5;241m200\u001b[39m, device)\n\u001b[0;32m     24\u001b[0m test_metrics_run \u001b[38;5;241m=\u001b[39m evaluate_model(trained_model, dim, test_loader, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric, value \u001b[38;5;129;01min\u001b[39;00m test_metrics_run\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[1;32mIn[6], line 18\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, loss_fn, optimizer, num_epochs, device, early_stopping_patience, print_statements)\u001b[0m\n\u001b[0;32m     15\u001b[0m train_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     16\u001b[0m train_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     19\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     20\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n",
      "File \u001b[1;32mc:\\Users\\migue\\anaconda3\\envs\\MML-CW\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\migue\\anaconda3\\envs\\MML-CW\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\migue\\anaconda3\\envs\\MML-CW\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\migue\\anaconda3\\envs\\MML-CW\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[38;5;241m=\u001b[39mdefault_collate_fn_map)\n",
      "File \u001b[1;32mc:\\Users\\migue\\anaconda3\\envs\\MML-CW\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 212\u001b[0m         collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\migue\\anaconda3\\envs\\MML-CW\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\migue\\anaconda3\\envs\\MML-CW\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(batch, \u001b[38;5;241m0\u001b[39m, out\u001b[38;5;241m=\u001b[39mout)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_true_list = []\n",
    "y_pred_proba_list = []\n",
    "\n",
    "for i, dim in enumerate(dimensionality):\n",
    "\tprint(f\"-=-=-=-= Running {dim} =-=-=-=-\")\n",
    "\n",
    "\tX_train = np.load(f'../../Datasets_Train_Test_Split/kryptonite-{dim}-X_train.npy')\n",
    "\tX_test = np.load(f'../../Datasets_Train_Test_Split/kryptonite-{dim}-X_test.npy')\n",
    "\ty_train = np.load(f'../../Datasets_Train_Test_Split/kryptonite-{dim}-y_train.npy')\n",
    "\ty_test = np.load(f'../../Datasets_Train_Test_Split/kryptonite-{dim}-y_test.npy')\n",
    "\n",
    "\tX_train = denoise_bernoulli_data(X_train)\n",
    "\tX_test = denoise_bernoulli_data(X_test)\n",
    "\n",
    "\tX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\tbest_params = best_params_by_dim[i]\n",
    "\ttrain_loader = create_dataloaders(X_train, y_train, best_params['batch_size'])\n",
    "\tval_loader = create_dataloaders(X_val, y_val, best_params['batch_size'])\n",
    "\ttest_loader = create_dataloaders(X_test, y_test, best_params['batch_size'])\n",
    "\tmodel = DynamicNeuralNetwork(dim, best_params['hidden_sizes'], 0.15).to(device)\n",
    "\toptimizer = torch.optim.Adam(model.parameters(), lr=best_params['learning_rate'], weight_decay=0.0001)\n",
    "\tloss_fn = torch.nn.BCELoss()\n",
    "\ttrained_model, _, _, _, _, _ = train_model(model, train_loader, val_loader, loss_fn, optimizer, 200, device)\n",
    "\ttest_metrics_run = evaluate_model(trained_model, dim, test_loader, False)\n",
    "\n",
    "\tfor metric, value in test_metrics_run.items():\n",
    "\t\tprint(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "\tpredictions = get_predictions_prob(trained_model, test_loader)\n",
    "\ty_true_list.append(predictions['all_labels'])\n",
    "\ty_pred_proba_list.append(predictions['all_predictions'])\n",
    "\n",
    "\tplot_roc_curve(predictions['all_labels'], predictions['all_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_multi_roc_curves(y_true_list, y_pred_proba_list, dimensionality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to run cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-=-=-=-= Running 9 =-=-=-=-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [00:36<03:25,  2.42s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m X \u001b[38;5;241m=\u001b[39m denoise_bernoulli_data(X)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Get both metrics and standard deviations\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m avg_metrics, std_metrics \u001b[38;5;241m=\u001b[39m k_fold_cross_validation(X, y, best_params, dim, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFinal K-Fold Cross Validation Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ± \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 15\u001b[0m, in \u001b[0;36mk_fold_cross_validation\u001b[1;34m(X, y, best_params, dim, k)\u001b[0m\n\u001b[0;32m     12\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m], weight_decay\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     13\u001b[0m     loss_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[1;32m---> 15\u001b[0m     trained_model, _, _, _, _, _ \u001b[38;5;241m=\u001b[39m train_model(model, train_loader, val_loader, loss_fn, optimizer, \u001b[38;5;241m100\u001b[39m, device)\n\u001b[0;32m     16\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(evaluate_model(trained_model, dim, val_loader, \u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m     18\u001b[0m metric_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[6], line 18\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, loss_fn, optimizer, num_epochs, device, early_stopping_patience, print_statements)\u001b[0m\n\u001b[0;32m     15\u001b[0m train_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     16\u001b[0m train_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     19\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     20\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n",
      "File \u001b[1;32mc:\\Users\\migue\\anaconda3\\envs\\MML-CW\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\migue\\anaconda3\\envs\\MML-CW\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\migue\\anaconda3\\envs\\MML-CW\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\migue\\anaconda3\\envs\\MML-CW\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[38;5;241m=\u001b[39mdefault_collate_fn_map)\n",
      "File \u001b[1;32mc:\\Users\\migue\\anaconda3\\envs\\MML-CW\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 212\u001b[0m         collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\migue\\anaconda3\\envs\\MML-CW\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\migue\\anaconda3\\envs\\MML-CW\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(batch, \u001b[38;5;241m0\u001b[39m, out\u001b[38;5;241m=\u001b[39mout)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, dim in enumerate(dimensionality):\n",
    "\tprint(f\"-=-=-=-= Running {dim} =-=-=-=-\")\n",
    "\n",
    "\tbest_params = best_params_by_dim[i]\n",
    "\n",
    "\tX = np.load(f'../../Datasets/kryptonite-{dim}-X.npy')\n",
    "\ty = np.load(f'../../Datasets/kryptonite-{dim}-y.npy')\n",
    "\n",
    "\tX = denoise_bernoulli_data(X)\n",
    "\n",
    "\t# Get both metrics and standard deviations\n",
    "\tavg_metrics, std_metrics = k_fold_cross_validation(X, y, best_params, dim, k=2)\n",
    "\n",
    "\tprint(\"\\nFinal K-Fold Cross Validation Results:\")\n",
    "\tprint(f\"Average Accuracy: {avg_metrics['accuracy']:.4f} ± {std_metrics['accuracy']:.4f}\")\n",
    "\tprint(f\"Average Precision: {avg_metrics['precision']:.4f} ± {std_metrics['precision']:.4f}\")\n",
    "\tprint(f\"Average Recall: {avg_metrics['recall']:.4f} ± {std_metrics['recall']:.4f}\")\n",
    "\tprint(f\"Average F1: {avg_metrics['f1']:.4f} ± {std_metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the predicitons of the hidden data up to 18 dimesnions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\.'\n",
      "C:\\Users\\migue\\AppData\\Local\\Temp\\ipykernel_20368\\1454967782.py:5: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  X = np.load(f'..\\..\\Datasets\\kryptonite-{dim}-X.npy')\n",
      "C:\\Users\\migue\\AppData\\Local\\Temp\\ipykernel_20368\\1454967782.py:6: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  y = np.load(f'..\\..\\Datasets\\kryptonite-{dim}-y.npy')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-=-=-=-= Running 9 =-=-=-=-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 64/200 [03:04<07:09,  3.16s/it]"
     ]
    }
   ],
   "source": [
    "dimensionality = [9, 12, 15, 18]\n",
    "\n",
    "for i, dim in enumerate(dimensionality):\n",
    "\tprint(f\"-=-=-=-= Running {dim} =-=-=-=-\")\n",
    "\tX = np.load(f'..\\..\\Datasets\\kryptonite-{dim}-X.npy')\n",
    "\ty = np.load(f'..\\..\\Datasets\\kryptonite-{dim}-y.npy')\n",
    "\n",
    "\tX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\tX_train = np.array(denoise_bernoulli_data(X_train))\n",
    "\tX_val = np.array(denoise_bernoulli_data(X_val))\n",
    "\n",
    "\tbest_params = best_params_by_dim[i] # The best parameter for dim = 18 is used for the higher dimensions\n",
    "\n",
    "\t# Convert to PyTorch tensors\n",
    "\tX_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "\ty_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "\tX_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "\ty_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "\n",
    "\t# Create data loaders\n",
    "\ttrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\tval_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\ttrain_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\tval_loader = DataLoader(val_dataset, batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "\t# Initialize and train model\n",
    "\tmodel = DynamicNeuralNetwork(dim, best_params['hidden_sizes'], 0.15).to(device)\n",
    "\toptimizer = torch.optim.Adam(model.parameters(), lr=best_params['learning_rate'], weight_decay=0.0001)\n",
    "\tloss_fn = torch.nn.BCELoss()\n",
    "\ttrained_model, _, _, _, _, _ = train_model(model, train_loader, val_loader, loss_fn, optimizer, 200, device)\n",
    "\n",
    "\tX = np.load(rf'..\\Kryptonite-N-main\\Datasets\\hidden-kryptonite-{dim}-X.npy')\n",
    "\tX = denoise_bernoulli_data(X)\n",
    "\tX = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "    \n",
    "\tpredict_dataset = TensorDataset(X)\n",
    "\tpredict_loader = DataLoader(predict_dataset, batch_size=best_params['batch_size'], shuffle=False)\n",
    "    \n",
    "\tpredictions = get_predictions_final(model, predict_loader)\n",
    "\tnp.save(f'nn_predictions_{dim}.npy', predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the predicitons of the hidden data dim > 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensionality = [24, 30, 45]\n",
    "\n",
    "for i, dim in enumerate(dimensionality):\n",
    "\tprint(f\"-=-=-=-= Running {dim} =-=-=-=-\")\n",
    "\tX = np.load(rf'..\\Kryptonite-N-main\\Datasets\\kryptonite-{dim}-X.npy')\n",
    "\ty = np.load(rf'..\\Kryptonite-N-main\\Datasets\\kryptonite-{dim}-y.npy')\n",
    "\n",
    "\tX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\tX_train = np.array(denoise_bernoulli_data(X_train))\n",
    "\tX_val = np.array(denoise_bernoulli_data(X_val))\n",
    "\n",
    "\tbest_params = best_params_by_dim[i]\n",
    "\n",
    "\t# Convert to PyTorch tensors\n",
    "\tX_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "\ty_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "\tX_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "\ty_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "\n",
    "\t# Create data loaders\n",
    "\ttrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\tval_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\ttrain_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\tval_loader = DataLoader(val_dataset, batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "\t# Initialize and train model\n",
    "\tmodel = DynamicNeuralNetwork(dim, best_params['hidden_sizes'], 0.15).to(device)\n",
    "\toptimizer = torch.optim.Adam(model.parameters(), lr=best_params['learning_rate'], weight_decay=0.0001)\n",
    "\tloss_fn = torch.nn.BCELoss()\n",
    "\ttrained_model, _, _, _, _, _ = train_model(model, train_loader, val_loader, loss_fn, optimizer, 200, device)\n",
    "\n",
    "\tX = np.load(rf'..\\Kryptonite-N-main\\Datasets\\hidden-kryptonite-{dim}-X.npy')\n",
    "\tX = denoise_bernoulli_data(X)\n",
    "\tX = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "    \n",
    "\tpredict_dataset = TensorDataset(X)\n",
    "\tpredict_loader = DataLoader(predict_dataset, batch_size=best_params['batch_size'], shuffle=False)\n",
    "    \n",
    "\tpredictions = get_predictions_final(model, predict_loader)\n",
    "\tnp.save(f'nn_predictions_{dim}.npy', predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MML-CW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
