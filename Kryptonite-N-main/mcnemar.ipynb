{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "McNemar test p-value: 0.0002771615982055664\n",
      "The performance difference between the models is statistically significant.\n",
      "[[126   3]\n",
      " [ 21   0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n",
    "# Generate a synthetic binary classification dataset\n",
    "X, y = make_classification(n_samples=500, n_features=20, random_state=42)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train[:,0:5], y_train)\n",
    "\n",
    "# Train Decision Tree Classifier\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions from both models\n",
    "lr_preds = lr_model.predict(X_test[:,0:5])\n",
    "dt_preds = dt_model.predict(X_test)\n",
    "\n",
    "# Build the contingency table\n",
    "# Both correct\n",
    "both_correct = np.sum((lr_preds == y_test) & (dt_preds == y_test))\n",
    "# Logistic correct, Decision Tree incorrect\n",
    "lr_correct_dt_incorrect = np.sum((lr_preds == y_test) & (dt_preds != y_test))\n",
    "# Logistic incorrect, Decision Tree correct\n",
    "lr_incorrect_dt_correct = np.sum((lr_preds != y_test) & (dt_preds == y_test))\n",
    "# Both incorrect\n",
    "both_incorrect = np.sum((lr_preds != y_test) & (dt_preds != y_test))\n",
    "\n",
    "# Contingency table\n",
    "contingency_table = np.array([[both_correct + both_incorrect, lr_correct_dt_incorrect],\n",
    "                              [lr_incorrect_dt_correct, 0]])\n",
    "\n",
    "# Perform McNemar test\n",
    "result = mcnemar(contingency_table, exact=True)\n",
    "\n",
    "# Output the test results\n",
    "print(\"McNemar test p-value:\", result.pvalue)\n",
    "if result.pvalue < 0.05:\n",
    "    print(\"The performance difference between the models is statistically significant.\")\n",
    "else:\n",
    "    print(\"The performance difference between the models is not statistically significant.\")\n",
    "\n",
    "print(contingency_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train-Test Split for McNemar Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: [9] (14400, 9) (3600, 9) (14400,) (3600,)\n",
      "n: [12] (19200, 12) (4800, 12) (19200,) (4800,)\n",
      "n: [15] (24000, 15) (6000, 15) (24000,) (6000,)\n",
      "n: [18] (28800, 18) (7200, 18) (28800,) (7200,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RUN_TRAIN_TEST_SPLIT = False\n",
    "\n",
    "list_n = [9, 12, 15, 18]\n",
    "\n",
    "save_loc = f'Datasets_McNemar_Test/'\n",
    "\n",
    "if RUN_TRAIN_TEST_SPLIT\n",
    "    for n in list_n:\n",
    "        X = np.load('Datasets/kryptonite-%s-X.npy' % (n))\n",
    "        y = np.load('Datasets/kryptonite-%s-y.npy' % (n))\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # 20% of all data for final model evaluation\n",
    "\n",
    "        print(f'n: [{n}]', X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "        np.save('%skryptonite-%s-X_train.npy' % (save_loc, n), X_train)\n",
    "        np.save('%skryptonite-%s-X_test.npy' % (save_loc, n), X_test)\n",
    "        np.save('%skryptonite-%s-y_train.npy' % (save_loc, n), y_train)\n",
    "        np.save('%skryptonite-%s-y_test.npy' % (save_loc, n), y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test from Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcnemar_test(y_true, y_pred_m1, y_pred_m2):\n",
    "    both_correct = np.sum((y_pred_m1 == y_true) & (y_pred_m2 == y_true))\n",
    "    # Logistic correct, Decision Tree incorrect\n",
    "    lr_correct_dt_incorrect = np.sum((y_pred_m1 == y_true) & (y_pred_m2 != y_true))\n",
    "    # Logistic incorrect, Decision Tree correct\n",
    "    lr_incorrect_dt_correct = np.sum((y_pred_m1 != y_true) & (y_pred_m2 == y_true))\n",
    "    # Both incorrect\n",
    "    both_incorrect = np.sum((y_pred_m1 != y_true) & (y_pred_m2 != y_true))\n",
    "\n",
    "    # Contingency table\n",
    "    contingency_table = np.array([[both_correct + both_incorrect, lr_correct_dt_incorrect],\n",
    "                                [lr_incorrect_dt_correct, 0]])\n",
    "\n",
    "    # Perform McNemar test\n",
    "    result = mcnemar(contingency_table, exact=True)\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
